#!/usr/bin/env python
import argparse
from pathlib import Path

import pandas as pd

from github_stats_pages import gts_run
from github_stats_pages.logger import app_log as log
from github_stats_pages import db


def read_csv(csv_file: str) -> pd.DataFrame:
    return pd.read_csv(csv_file)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="CLI to retrieve GitHub traffic statistics"
    )
    parser.add_argument(
        "-u", "--user", required=True, help="user or organization name"
    )
    parser.add_argument("-t", "--token", required=True, help="API token")
    parser.add_argument("-c", "--csv-file", required=True, help="CSV filename")
    parser.add_argument(
        "--test",
        action="store_true",
        help="Flag to quickly run a few repositories",
    )
    args = parser.parse_args()

    log.info("[bold yellow]Running gts_run_all_repos script")

    df = read_csv(args.csv_file)

    # Exclude forks and archived repos
    n_forks = df["fork"].values.sum()
    n_archived = df["archived"].values.sum()
    if n_forks > 0:
        log.info(f"Excluding forks: {n_forks}")
    if n_archived > 0:
        log.info(f"Excluding archived: {n_archived}")
    new_df = df.loc[(~df["fork"]) & (~df["archived"])]
    log.info(f"Number of repositories: {len(new_df)}")

    if args.test:
        repo_list = new_df["name"][0:5]
    else:
        repo_list = new_df["name"]

    for repo_name in repo_list:
        log.info(f"[yellow]Working on: {repo_name}")
        gts_run.run_each_repo(args.user, args.token, repo_name, save_csv=True)
        gts_run.get_top_paths(args.user, args.token, repo_name, save_csv=True)

    log.info("[yellow]Moving CSV records to SQLite")
    engine = db.create_db_and_tables()

    p_cwd = Path.cwd()
    p_data = p_cwd / "data"

    model_names = ["clone", "traffic", "paths", "referrer"]
    models = [db.Clone, db.Traffic, db.Paths, db.Referring]
    for datatype, model in zip(model_names, models):
        for f in p_cwd.glob(f"????-??-??-???-???-*{datatype}-stats.csv"):
            db.migrate_csv(f, model, engine, skip_rows=1)

    log.info("[bold dark_green]gts_run_all_repos script completed!")
